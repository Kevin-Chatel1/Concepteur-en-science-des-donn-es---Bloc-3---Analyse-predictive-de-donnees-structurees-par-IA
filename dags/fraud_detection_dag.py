from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from datetime import datetime, timedelta
from utils.common import *
import requests
import joblib
import importlib.util
import json
import os
from sqlalchemy import create_engine, text, Boolean
from sqlalchemy.exc import SQLAlchemyError
import mlflow

# Configuration des arguments par dÃ©faut pour le DAG
default_args = {
    'owner': 'fraud_team',
    'depends_on_past': False,
    'email_on_failure': True,
    'retries': 1,
    'retry_delay': timedelta(minutes=1),
}

# Fonction pour charger les dÃ©pendances depuis S3
def load_dependencies(**context):
    """Charge le modÃ¨le et l'ETL depuis S3"""
    try:
        logger.info("Chargement des dÃ©pendances depuis S3...")
        s3_client = get_s3_client()
        
        # CrÃ©ation du dossier temporaire pour stocker les fichiers tÃ©lÃ©chargÃ©s
        tmp_dir = '/tmp/fraud_detection'
        os.makedirs(tmp_dir, exist_ok=True)
        
        # Configuration des chemins S3 sans prÃ©fixes additionnels
        s3_paths = {
            'model': "models/random_forest_model.pkl",
            'etl': "etl/etl.py"
        }
        
        # TÃ©lÃ©chargement du modÃ¨le
        model_path = f"{tmp_dir}/model.pkl"
        s3_client.download_file(S3_BUCKET, s3_paths['model'], model_path)
        logger.info("ModÃ¨le chargÃ© avec succÃ¨s")
        
        # TÃ©lÃ©chargement de l'ETL
        etl_path = f"{tmp_dir}/etl.py"
        s3_client.download_file(S3_BUCKET, s3_paths['etl'], etl_path)
        logger.info("ETL chargÃ© avec succÃ¨s")
        
        # Stockage des chemins dans XCom
        context['task_instance'].xcom_push(key='model_path', value=model_path)
        context['task_instance'].xcom_push(key='etl_path', value=etl_path)
        
        return True
        
    except Exception as e:
        logger.error(f"Erreur lors du chargement des dÃ©pendances: {str(e)}")
        raise

def fetch_api(**context):
    try:
        response = requests.get(API_URL)
        response.raise_for_status()
        transactions = response.json()
        logger.info("Transaction reÃ§ue avec succÃ¨s")
        context['ti'].xcom_push(key='transactions', value=transactions)

    except Exception as e:
        logger.error(f"Erreur de connexion Ã  l'API: {str(e)}")
        raise

def process_transaction(**context):
    try:
        # RÃ©cupÃ©ration des donnÃ©es avec une meilleure gestion d'erreur
        task_instance = context['task_instance']
        raw_transactions = task_instance.xcom_pull(task_ids='fetch_api', key='transactions')
        
        logger.info(f"DonnÃ©es rÃ©cupÃ©rÃ©es depuis XCom: {raw_transactions}")

        if not raw_transactions:
            logger.warning("Pas de donnÃ©es Ã  traiter")
            return 'skip_processing'
        
        # Parsing JSON
        transactions = json.loads(raw_transactions)
        logger.info("DonnÃ©es JSON parsÃ©es avec succÃ¨s")
            
        # CrÃ©ation du DataFrame
        df = pd.DataFrame(transactions['data'], columns=transactions['columns'])
        logger.info(f"DataFrame crÃ©Ã© avec {len(df)} lignes")
        
        # Renommage de la colonne current_time en trans_date_trans_time
        df = df.rename(columns={'current_time': 'trans_date_trans_time'})
        logger.info("Colonnes renommÃ©es avec succÃ¨s")
        logger.info(f"Colonnes disponibles: {df.columns.tolist()}")

        # Conversion en timestamp Unix (en secondes)
        df['unix_time'] = df['trans_date_trans_time'].astype('int64') // 10**9  # Conversion en secondes
        logger.info("Conversion de trans_date_trans_time en unix_time rÃ©ussie")
        
        # Import de l'ETL
        etl_path = context['ti'].xcom_pull(task_ids='load_dependencies', key='etl_path')
        spec = importlib.util.spec_from_file_location("etl", etl_path)
        etl = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(etl)
        
        # Transformation des donnÃ©es
        logger.info("Transformation des donnÃ©es...")
        df_transformed = etl.transform_data(df)
        
        # Chargement et prÃ©diction avec le modÃ¨le
        model_path = context['ti'].xcom_pull(task_ids='load_dependencies', key='model_path')
        model = joblib.load(model_path)
        
        # PrÃ©diction
        prediction = model.predict(df_transformed)[0]
        probability = model.predict_proba(df_transformed)[:, 1][0]

        # Ajout du logging pour la prÃ©diction et la probabilitÃ©
        logger.info(f"Prediction de fraude: {'FRAUDE' if prediction == 1 else 'NORMAL'}, ProbabilitÃ©: {probability:.2%}")
        
        # Stockage des rÃ©sultats
        context['ti'].xcom_push(key='is_fraud', value=bool(prediction))
        context['ti'].xcom_push(key='fraud_probability', value=float(probability))

        logger.info(f"PrÃ©diction: {'FRAUDE' if prediction == 1 else 'NORMAL'} (ProbabilitÃ©: {probability:.2%})")
        
        # version Ã  remettre quand email sera verif : return 'send_fraud_alert' if prediction == 1 else 'store_normal'
        return 'notify_fraud' if prediction == 1 else 'notify_normal'

    except Exception as e:
        logger.error(f"Erreur lors du traitement: {str(e)}")
        logger.error(f"Type des transactions: {type(raw_transactions)}")
        logger.error(f"Contenu des transactions: {str(raw_transactions)[:200]}...")
        return 'skip_processing' 

def send_transaction_email(**context):
    try:
        # RÃ©cupÃ©ration des donnÃ©es
        transaction = context['task_instance'].xcom_pull(task_ids='fetch_api', key='transactions')
        probability = context['task_instance'].xcom_pull(key='fraud_probability')
        is_fraud = context['task_instance'].xcom_pull(key='is_fraud', default=False)
        
        logger.info(f"DonnÃ©es de transaction rÃ©cupÃ©rÃ©es: {transaction}")
        logger.info(f"ProbabilitÃ© de fraude: {probability}")
        logger.info(f"Est une fraude: {is_fraud}")
        
        # VÃ©rification des donnÃ©es
        if transaction is None:
            logger.error("Aucune donnÃ©e de transaction trouvÃ©e dans XCom.")
            return 'store_normal' if not is_fraud else 'store_fraud'

        if probability is None:
            logger.error("Aucune probabilitÃ© de fraude trouvÃ©e dans XCom.")
            return 'store_normal' if not is_fraud else 'store_fraud'
            
        # Parser le JSON si nÃ©cessaire
        if isinstance(transaction, str):
            transaction = json.loads(transaction)
            
        # Convertir en DataFrame pour faciliter l'accÃ¨s aux donnÃ©es
        df = pd.DataFrame(transaction['data'], columns=transaction['columns']).iloc[0]
        
        # Formatage du montant avec 2 dÃ©cimales
        amount = "{:.2f}".format(float(df['amt']))
        
        # Formatage de la date
        transaction_date = pd.to_datetime(df['current_time'], unit='ms').strftime('%Y-%m-%d %H:%M:%S')

        # Construction du sujet et du contenu en fonction du type de transaction
        if is_fraud:
            subject = "ðŸš¨ ALERTE: Transaction frauduleuse dÃ©tectÃ©e!"
            status_color = "Rouge"
            status_text = "FRAUDULEUSE"
        else:
            subject = "âœ… INFO: Transaction normale dÃ©tectÃ©e"
            status_color = "Vert"
            status_text = "NORMALE"
        
        body = f"""
        {'ðŸš¨' if is_fraud else 'âœ…'} Transaction {status_text} dÃ©tectÃ©e!
        
        Statut: {status_color}
        ProbabilitÃ© de fraude: {probability:.2%}
        
        DÃ©tails de la transaction:
        --------------------------
        ID Transaction: {df['trans_num']}
        Montant: {amount}â‚¬
        Date/Heure: {transaction_date}
        
        Informations sur le marchand:
        ----------------------------
        Nom: {df['merchant']}
        Ville: {df['city']}
        Ã‰tat: {df['state']}
        
        Informations sur le client:
        --------------------------
        Nom: {df['first']} {df['last']}
        Ville: {df['city']}
        
        Cette notification a Ã©tÃ© gÃ©nÃ©rÃ©e automatiquement par le systÃ¨me de dÃ©tection de fraude.
        """
        
        logger.info(f"Tentative d'envoi d'email {status_text.lower()}...")
        logger.info(f"Contenu de l'email:\n{body}")
        
        # Utilisation de la fonction send_email de common.py
        success = send_email(
            subject=subject, 
            body=body
            # On n'envoie pas to_email pour utiliser l'email par dÃ©faut
        )
        
        if success:
            logger.info("Email envoyÃ© avec succÃ¨s")
        else:
            logger.error("Ã‰chec de l'envoi de l'email")
            
        return 'store_normal' if not is_fraud else 'store_fraud'
        
    except Exception as e:
        logger.error(f"Erreur lors de l'envoi de l'email: {str(e)}")
        return 'store_normal' if not is_fraud else 'store_fraud'

def store_transaction(**context):
    try:
        # Test de connexion Ã  Neon
        db_url = os.environ.get('NEON_DATABASE_URL')
        if not db_url:
            logger.error("NEON_DATABASE_URL n'est pas dÃ©finie")
            raise ValueError("NEON_DATABASE_URL manquante")
            
        logger.info("Tentative de connexion Ã  Neon...")
        engine = create_engine(db_url)
        
        # Test simple de connexion
        with engine.connect() as connection:
            result = connection.execute(text("SELECT 1"))
            logger.info("Connexion Ã  Neon rÃ©ussie!")
        
        # RÃ©cupÃ©ration et parsing des donnÃ©es
        raw_transactions = context['task_instance'].xcom_pull(
            task_ids='fetch_api', 
            key='transactions'
        )
        logger.info(f"DonnÃ©es reÃ§ues pour stockage: {raw_transactions}")
        
        if not raw_transactions:
            logger.error("Aucune donnÃ©e de transaction trouvÃ©e dans XCom.")
            return
            
        # Parser le JSON si c'est une chaÃ®ne
        if isinstance(raw_transactions, str):
            raw_transactions = json.loads(raw_transactions)
            logger.info("JSON parsÃ© avec succÃ¨s")
        
        # CrÃ©ation du DataFrame
        df = pd.DataFrame(raw_transactions['data'], columns=raw_transactions['columns'])
        logger.info(f"DataFrame crÃ©Ã© avec {len(df)} lignes")

        # Renommer la colonne current_time en trans_date_trans_time
        if 'current_time' in df.columns:
            df = df.rename(columns={'current_time': 'trans_date_trans_time'})
            logger.info("Colonne current_time renommÃ©e en trans_date_trans_time")
        
        # Convertir le timestamp en datetime
        df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'], unit='ms')
        logger.info("Conversion du timestamp rÃ©ussie")

        # RÃ©cupÃ©ration de l'indicateur de fraude depuis XCom
        is_fraud = context['task_instance'].xcom_pull(key='is_fraud', default=False)
        logger.info(f"Indicateur de fraude rÃ©cupÃ©rÃ©: {is_fraud}")

        # Mise Ã  jour de la colonne is_fraud dans le DataFrame
        df['is_fraud'] = is_fraud
        logger.info(f"Colonne is_fraud mise Ã  jour avec la valeur: {is_fraud}")

        # Convertir is_fraud en boolean
        df['is_fraud'] = df['is_fraud'].astype(bool)
        logger.info("Conversion de is_fraud en boolean rÃ©ussie")

        # Log des colonnes et types
        logger.info(f"Colonnes finales avant stockage: {df.columns.tolist()}")
        logger.info(f"Types des donnÃ©es: \n{df.dtypes}")

        # DÃ©termination de la table cible
        table = 'fraud_transactions' if is_fraud else 'normal_transactions'
        logger.info(f"Stockage dans la table: {table}")
        
        # Stockage des donnÃ©es avec dtype spÃ©cifiÃ© pour is_fraud
        dtype = {'is_fraud': Boolean}
        df.to_sql(table, engine, if_exists='append', index=False, dtype=dtype)
        logger.info(f"DonnÃ©es stockÃ©es avec succÃ¨s dans la table {table}")

    except SQLAlchemyError as e:
        logger.error(f"Erreur SQLAlchemy: {str(e)}")
        if 'df' in locals():
            logger.error(f"Colonnes du DataFrame: {df.columns.tolist()}")
            logger.error(f"Types des donnÃ©es: \n{df.dtypes}")
        raise
    except Exception as e:
        logger.error(f"Erreur inattendue: {str(e)}")
        logger.error(f"Type des donnÃ©es reÃ§ues: {type(raw_transactions)}")
        raise

def store_normal(**context):
    """Wrapper pour stocker les transactions normales"""
    context['task_instance'].xcom_push(key='is_fraud', value=False)
    return store_transaction(**context)

def store_fraud(**context):
    """Wrapper pour stocker les transactions frauduleuses"""
    context['task_instance'].xcom_push(key='is_fraud', value=True)
    return store_transaction(**context)

def configure_mlflow():
    """Configure MLflow et force l'utilisation d'une run unique."""
    mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI", "http://mlflow:5000"))
    experiment_name = "fraud_detection_pipeline"

    # VÃ©rifie si l'expÃ©rience existe dÃ©jÃ 
    client = mlflow.tracking.MlflowClient()
    experiment = client.get_experiment_by_name(experiment_name)

    if experiment is None:
        # CrÃ©e l'expÃ©rience si elle n'existe pas
        mlflow.create_experiment(name=experiment_name)
    mlflow.set_experiment(experiment_name)

def log_to_mlflow(**context):
    """Log les rÃ©sultats dans MLflow, en Ã©crasant les artefacts prÃ©cÃ©dents."""
    try:
        configure_mlflow()
        client = mlflow.tracking.MlflowClient()

        # Nom de l'expÃ©rience et du run
        experiment_name = "fraud_detection_pipeline"
        artifact_path = "model"
        fixed_run_name = "fraud_detection_fixed_run"

        # VÃ©rifier ou crÃ©er l'expÃ©rience
        experiment = client.get_experiment_by_name(experiment_name)
        if experiment is None:
            logger.info(f"CrÃ©ation de l'expÃ©rience {experiment_name}.")
            experiment_id = client.create_experiment(experiment_name)
        else:
            experiment_id = experiment.experiment_id

        # Supprimer les artefacts existants dans S3
        s3_client = get_s3_client()
        bucket_name = os.environ.get("S3_BUCKET")
        prefix = f"mlartifacts/{artifact_path}/"
        logger.info(f"Suppression des artefacts existants dans le bucket {bucket_name} au prÃ©fixe {prefix}...")
        
        objects_to_delete = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix).get('Contents', [])
        if objects_to_delete:
            delete_request = {
                'Objects': [{'Key': obj['Key']} for obj in objects_to_delete],
                'Quiet': True
            }
            s3_client.delete_objects(Bucket=bucket_name, Delete=delete_request)
            logger.info("Artefacts existants supprimÃ©s avec succÃ¨s.")
        else:
            logger.info("Aucun artefact Ã  supprimer.")

        # CrÃ©er un nouveau run
        logger.info(f"CrÃ©ation d'un nouveau run avec le nom {fixed_run_name}.")
        with mlflow.start_run(experiment_id=experiment_id, run_name=fixed_run_name):
            # RÃ©cupÃ©rer les donnÃ©es de contexte
            is_fraud = context['task_instance'].xcom_pull(key='is_fraud')
            fraud_probability = context['task_instance'].xcom_pull(key='fraud_probability')

            # Enregistrer les mÃ©triques et paramÃ¨tres
            mlflow.log_param("model_version", "RandomForest")
            mlflow.log_metric("fraud_probability", fraud_probability)
            mlflow.log_metric("is_fraud", int(is_fraud))

            # Enregistrer les artefacts
            mlflow.log_artifact("/tmp/fraud_detection/model.pkl", artifact_path="model")
            mlflow.log_artifact("/tmp/fraud_detection/etl.py", artifact_path="etl")

        logger.info("Run MLflow crÃ©Ã© et loguÃ© avec succÃ¨s.")

    except Exception as e:
        logger.error(f"Erreur lors du logging dans MLflow : {str(e)}")
        raise

with DAG(
    'fraud_detection',
    default_args=default_args,
    description='DÃ©tection de fraude en temps rÃ©el',
    schedule_interval='* * * * *',
    start_date=datetime(2024, 1, 1),
    catchup=False
) as dag:
    
    # Chargement des dÃ©pendances
    load_deps = PythonOperator(
        task_id='load_dependencies',
        python_callable=load_dependencies,
    )

    # RÃ©cupÃ©ration des donnÃ©es de l'API
    fetch_api = PythonOperator(
        task_id='fetch_api',
        python_callable=fetch_api,
        do_xcom_push=True,
    )

    # Traitement et prÃ©diction
    process = BranchPythonOperator(
        task_id='process_transaction',
        python_callable=process_transaction,
    )

    # Email et stockage pour transactions normales
    notify_normal = PythonOperator(
        task_id='notify_normal',
        python_callable=send_transaction_email,
        provide_context=True
    )

    store_normal = PythonOperator(
        task_id='store_normal',
        python_callable=store_transaction,
        provide_context=True
    )

    # Email et stockage pour transactions frauduleuses
    notify_fraud = PythonOperator(
        task_id='notify_fraud',
        python_callable=send_transaction_email,
        provide_context=True
    )

    store_fraud = PythonOperator(
        task_id='store_fraud',
        python_callable=store_transaction,
        provide_context=True
    )

    skip = PythonOperator(
        task_id='skip_processing',
        python_callable=lambda: logger.info("Pas de donnÃ©es")
    )

    log_metrics = PythonOperator(
    task_id='log_to_mlflow',
    python_callable=log_to_mlflow,
    provide_context=True,
    trigger_rule='none_failed_min_one_success'  # ExÃ©cution si une tÃ¢che rÃ©ussit
)


    # DÃ©finition des dÃ©pendances
    load_deps >> fetch_api >> process >> [notify_normal, notify_fraud, skip]
    notify_normal >> store_normal >> log_metrics
    notify_fraud >> store_fraud >> log_metrics

    # DÃ©finition des dÃ©pendances
    load_deps >> fetch_api >> process >> [notify_normal, notify_fraud, skip]
    notify_normal >> store_normal
    notify_fraud >> store_fraud

    # L'Ã©tape log_to_mlflow
    [store_normal, store_fraud] >> log_metrics

